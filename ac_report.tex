\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Skip prediction using decision trees\\}

\author{\IEEEauthorblockN{Jasmijn Bookelmann}
\IEEEauthorblockA{\textit{Radboud University} \\
Nijmegen, Netherlands \\
jasmijn.bookelmann@ru.nl}
}

\maketitle

\begin{abstract}
	NOTE THAT THIS IS AN ASSIGNMENT PAPER NOT BASED ON ACTUAL RESEARCH - 
	Skip prediction is predicting whether a user will skip a song or not. 
	This is a good indication of how much the user likes this song.
	Therefore, it is an important component of recommender systems. Such as the spotify algorithm. 
	We have created a skip prediction model using the spotify sessions database, 
	which contains listening sessions and metadata of the songs in it. 
	Our model predicts based on this initial session whether a user will skip the next song or not.
	For creating this model, we have first preprocessed the data by summarizing the session into unary variables. 
	After this we apply decision trees to classify this data.
\end{abstract}

\section{Introduction}
Automatic recommendations are getting increasingly popular with the digitalisation of music. They have an important role in the consumption of music nowadays. 

Predicting whether or not a user will skip a song is known as skip prediction. 
Accurate skip prediction is a tool in automatic recommendation systems, as it is
a measure for whether a user will enjoy a song\cite{b1}. 
This can be used in systems such as the playlist creator or autoplay from Spotify. 

We evaluate the performance of skip prediction using decision trees.

\section{Background}
In 2019 spotify released a session dataset of around 10GB filled with anonimized user listening sessions \cite{b2}. We will use this dataset in order to create, train and test our model. 

Every record in this dataset has two main parts: The listening session and the track. 
The goal of our model is to predict whether the track will be skipped based on the listening session and its metadata.

\subsection{Listening Sessions}
The listening sessions contain the tracks in the order the user listened to. These sessions contain up to 20 tracks.

The session records have the following properties: 
Whether the user has Spotify Premium or not, 
the action causing the listening session to start, 
the date, etc. 

\subsection{Tracks}
The track entries contain data about their audio features, 
provided by the spotify API\@. 
This is information such as the bounciness, dancability and key.

In addition, the duration, popularity and release year.
A track is skipped if a user did not listen to the entire track. There are three metrics which measure this: 
\begin{itemize}
	\item \verb|skip_1|: The track was only played very briefly
	\item \verb|skip_2|: The track was only played briefly
	\item \verb|skip_3|: Most of the track was played
	\item \verb|not_skipped|: The track was played in its entirety
\end{itemize}
We use \verb|skip_2| as ground-truth.


\section{Methods}
We reduce our research question to a classification problem for our model. 
This question is formulated as follows: Assuming we have session data containing 9 songs and the track information of the 10th song, classify this track as `skipped' or `not skipped'. 


We first preprocess the data such that it can be used to train our model.
After this we find the optimal parameters for our model using k-fold cross validation.
We use smaller subsets of the data with roughly 100.000 entries to train and test our model 
as we have limited resources. 

\subsection{Preprocessing}
In this section we discuss the features of every record which are used by our data mining model.
In section A we discuss the features of the current track, in B and C we discuss how we summarize the data from the session.

\subsubsection{Track Features}
As mentioned in the Background section, every track has 21 audio features.
As seen in table 3.1 the ranges of track features differ quite a lot.
We have standardized this data, which avoids certain track features with larger ranges having larger impact on our model. 


In addition to the audio features, there is also the \verb|duration|, \verb|release_year| and 
\verb|popularity|.
We do not change this data.

\subsubsection{Session Skips}
Our models do not take sequential data into account, 
thus we summarize the session skips into single features. 
These are the features of every track in the session indicating whether the user skipped this track or not.
In order to use this for calculations we need to convert the skip to a number. 
We use \verb|skip_2| as indicator of whether a user skipped or not in a track, 
just like the ground-truth. 
If \verb|skip_2| is true, then our skip number equals $1$, 
otherwise is $0$.

In order to summarize this we create two features:
\begin{itemize}
	\item \verb|%_skip|: The percentage of skips. 
		E.g., if a user has skipped 3 out of 10 tracks \verb|%_skip| is equal to $30\%$
	\item \verb|sd_skip|: The standard deviation of the skips. 
	So if a user has skipped none of the tracks \verb|sd_skip| is equal to 0. 
	Or if a user has skipped 3/10 tracks the \verb|sd_skip| is equal to $0.46$.
	This is a measure of how irregular the user's skipping behavior is.
\end{itemize}


\subsubsection{Session Accoustic Features}
In order to summarize the accoustic data of the session tracks we create two features. 
One feature is the average accoustic data of all the skipped tracks. 
The other feature is the average accoustic data of all the unskipped tracks.
This allows us to see the average information of the preferenced and unpreferenced tracks.

\subsubsection{Transforming other features}
In order to make the feature more representative of the data we also adjust the following features:

We reformat the date to represent the 7 days of the week instead of an absolute date. 
For example July 31st 2020 is rewritten to Friday. 
This will reduce the entropy of this feature, which is known to improve the 
performance of decision trees \cite{b1}. 
In addition, we expect this information to be more indicative of skipping behavior
than the absolute date.

Popularity, year of release and duration wil be kept the same.

\subsection{Sample selection}
As the data itself contains millions of tracks, and realistically we are not able to 
process this much using our limited resources. We select sample of our data
to train and test our model. 
These subsets each contain 100.000 track sessions. 

In order to select representative samples we apply stratified data sampling. 
As there is an uneven distribution of skipped tracks compared to non skipped, using this method we create an evenly distributed dataset. 
This will avoid pitfalls such as underrepresentation.

We select only one sample to train and select our model on. 
Then we test this model on multiple samples to ensure it performs the same 
accross different subsets of the data.

\subsection{Model Training}
\subsubsection{Performance Metric}
In order to measure the performance of our model we use the F-measure metric.
Here the accuracy is defined as:
\[
	\text{F-measure(F)} = \frac{2 \textit{TP}}{2 \textit{TP} + \textit{FP} + \textit{FN}}
\]

Where:
\begin{itemize}
	\item \textit{TP} = Cost of a true positive
	\item \textit{FP} = Cost of a false positive
	\item \textit{FN} = Cost of a false negative
\end{itemize}

In our accuracy metric we make the assumption that the cost of a false positive (recommending a song a user actually does not like) is worse than the cost of a false negative (not recommending a song a user might like).

From this we create the following cost matrix: \\
\verb|table of cost matrix|

In this matrix the cost of a false positive is 1.5 times this of a false negative.


\subsubsection{Model}
In order to train and apply our model, we use the \verb|DecisionTreeClassifier| from 
the python library scikit-learn. The advantages of this model are that it is easy to use and gives us a lot of insights in the inner workings of the decision tree. 
In addition, it has good compatability with plotting libraries such as matplotlib.

In order to optimize the parameters for the decision trees we apply nested k-fold cross validation. 

K-fold cross validation is a method which given a $k$, splits the dataset into $k$ groups. 
Then using one of the groups as training set and the others as test set, it measures the performance of the model. 
We set $k = 10$, which balances a decent amount of testing with a doable running time.

We apply cross validation for the following parameter ranges: 
\begin{itemize}
	\item max depth: 10, 15, 20, 25 
	\item min samples split: 15, 20, 30
	\item min weight fraction leaf: 0 1 3 5
\end{itemize}
In total we trained 48 models to check which parameters are optimal. 
This resulted in max depth = 15, min samples split = 30, min weight fraction leaf = 0. With a model of 85\% accuracy.

\section{Results}
Our model achieved an 85\% accuracy using F-measure. 
And an AUC score of 79\%.
This indicates that it was able to accurately predict whether a user will skip a song or not in most cases, with a bias towards true positives.
When applying our model to other samples the accuracy stays the same, which shows our model performs well on other samples too.

After mapping out the most influential features of our decision trees we see that US Popularity, Dancability and Average Session Skip have been the most influential in the classification.

Overall, our study has demonstrated the effectiveness of using decision tree models for predicting user behavior on Spotify, and has provided valuable insights for improving the user experience on the platform.

\verb|[show plot with most influential features and percentages]| \\
\verb|[show plot of ROC curve]| \\

\section{Discussion}
We have found that decision trees are effective when applying to skip prediction. 
Their flexibility allowed for a good prediction model.

We do have a few limitations: We had to train and test the model on relatively small sets of data due to limited resources.  
Thus further research could be conducted on larger sets of data to improve accuracy given stronger resources. 

In addition, by averaging the data of the sessions we lose a lot of sequential information. We believe sequence-based data training models, such as sequential neural networking, might result in better accuracy.

We believe further research can also delve more into machine learning algorithms using decision trees, such as random forests or boosting trees.

\section{Conclusion}
In conclusion, our study has demonstrated the effectiveness of using decision tree models for predicting user behavior on Spotify. The model was able to accurately predict whether a user would skip a song or not based on the acoustic information of the song and the songs in the listening session, with an average accuracy of 85%.

\section*{Acknowledgment}
I would like to thank Tim Kersten, Ivo Melse, Dani\"{e}l Mol and Rulian Wang for giving feedback on this paper.

\begin{thebibliography}{00}
\bibitem{b1} \verb|some relevant article|
\bibitem{b2} \verb|source of dataset|
\end{thebibliography}

\end{document}
